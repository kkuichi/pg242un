{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data = pd.read_csv(\"toxic_eng/train.csv\")\n",
    "test_data = pd.read_csv(\"toxic_eng/test.csv\")\n",
    "\n",
    "train_data['label'] = train_data.get('toxic')\n",
    "test_data['label'] = test_data.get('toxic')\n",
    "\n",
    "max_words = 20000  \n",
    "max_length = 128   \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['comment_text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['comment_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['comment_text'])\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "train_labels = np.array(train_data['label'])\n",
    "test_labels = np.array(test_data['label'])\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 128, input_length=max_length),\n",
    "    GRU(128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")  \n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_padded, train_labels, validation_data=(test_padded, test_labels), epochs=5, batch_size=32)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# ulozenie\n",
    "model.save(\"saved_gru_model.h5\")\n",
    "print(\"Model uložený ako saved_gru_model.h5\")\n",
    "\n",
    "# ulozenie tokenizera\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(\"tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(tokenizer_json)\n",
    "print(\"Tokenizer uložený ako tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "train_data = pd.read_csv(\"testy/toxic_eng/train.csv\")\n",
    "test_data = pd.read_csv(\"testy/toxic_eng/test.csv\")\n",
    "\n",
    "train_data['label'] = train_data.get('label', 0)\n",
    "test_data['label'] = test_data.get('label', 0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "train_inputs = tokenize_texts(train_data['comment_text']) \n",
    "test_inputs = tokenize_texts(test_data['comment_text'])\n",
    "\n",
    "train_labels = torch.tensor(train_data['label'].values)\n",
    "test_labels = torch.tensor(test_data['label'].values)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        gru_out = self.dropout(gru_out[:, -1, :])\n",
    "        output = self.fc(gru_out)\n",
    "        return output\n",
    "\n",
    "input_dim = tokenizer.vocab_size\n",
    "hidden_dim = 128\n",
    "output_dim = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = GRUModel(input_dim, hidden_dim, output_dim, num_layers, dropout)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir=\"checkpoints\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded: {checkpoint_path}, starting from epoch {epoch + 1}\")\n",
    "    return epoch\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=5, checkpoint_dir=\"checkpoints\", start_epoch=0):\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        save_checkpoint(model, optimizer, epoch, checkpoint_dir)\n",
    "\n",
    "checkpoint_path = \"checkpoints/checkpoint_epoch_2.pth\"  \n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_path) + 1\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, device, num_epochs=5, start_epoch=start_epoch)\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# predikcie modelu\n",
    "y_pred = model.predict(test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  \n",
    "\n",
    "# vypocet metrik\n",
    "precision = precision_score(test_labels, y_pred_binary)\n",
    "recall = recall_score(test_labels, y_pred_binary)\n",
    "f1 = f1_score(test_labels, y_pred_binary)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
