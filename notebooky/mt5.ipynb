{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e9887-10d2-40a7-9510-52c443324048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff703f3e-9a67-4bd2-9961-d3e1ab149089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de9b08-7909-40f8-af8e-e8da1cf1c1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e4161-bfa6-4090-a032-f9da958d945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8e1fb-dea3-4b3c-a3c2-9334029de84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629790b3-dadb-4000-aefe-84b4b7d6bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae8b42-244d-463f-a087-a74e6a7d6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import MT5Tokenizer, MT5ForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_df = pd.read_csv('toxic_eng/train.csv')\n",
    "test_df = pd.read_csv('toxic_eng/test.csv')\n",
    "\n",
    "train_texts = train_df['comment_text'].tolist()\n",
    "train_labels = train_df['toxic'].tolist()\n",
    "test_texts = test_df['comment_text'].tolist()\n",
    "test_labels = test_df['toxic'].tolist()\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "\n",
    "MAX_LEN = 128\n",
    "full_train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "# trenvovaciu mnozinu este dame na trenovaciu a validacnu\n",
    "train_size = int(0.9 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir, model_name, best_val_loss=None):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_epoch{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
    "\n",
    "def compute_val_loss(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# nacitame checkpoint len ked mame \n",
    "def load_checkpoint(model, optimizer, checkpoint_dir, model_name):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_best.pt\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"pokracjeme epohom{epoch + 1} s najlepsou validacnou stratou: {best_val_loss:.4f}\")\n",
    "        return epoch, best_val_loss\n",
    "    else:\n",
    "        print(\"nemame ziaden checkpoin ideme znova .\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, checkpoint_dir, model_name, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    epoch, best_val_loss = load_checkpoint(model, optimizer, checkpoint_dir, model_name)\n",
    "\n",
    "    for epoch in range(epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        val_loss = compute_val_loss(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, checkpoint_dir, model_name, best_val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_path = os.path.join(checkpoint_dir, f\"{model_name}_best.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, best_path)\n",
    "            print(f\"Best model saved at: {best_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Early stopping patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "\n",
    "def evaluate_mt5_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_tqdm:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MT5ForSequenceClassification.from_pretrained(\"google/mt5-base\", num_labels=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed80260-615a-4c56-8e0a-e7ad858fce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_early_stopping(\n",
    "    model, train_loader, val_loader,\n",
    "    criterion, optimizer,\n",
    "    device, num_epochs=10,\n",
    "    checkpoint_dir=\"checkpoints_full\",\n",
    "    model_name=\"mt5_toxicity_full\",\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "evaluate_mt5_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0e894-5d32-4af6-9c1d-bb20c2311e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from checkpoints_full/mt5_toxicity_full_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def load_best_model(model_class, checkpoint_dir, model_name, device):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_best.pt\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model = model_class.from_pretrained(\"google/mt5-base\", num_labels=2)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Best checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "best_model = load_best_model(MT5ForSequenceClassification, \"checkpoints_full\", \"mt5_toxicity_full\", device)\n",
    "evaluate_mt5_model(best_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab5420-16a6-43a3-8815-c9b34925da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from checkpoints_full/mt5_toxicity_full_epoch10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def load_best_model(model_class, checkpoint_dir, model_name, device):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}.pt\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model = model_class.from_pretrained(\"google/mt5-base\", num_labels=2)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Best checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "best_model = load_best_model(MT5ForSequenceClassification, \"checkpoints_full\", \"mt5_toxicity_full_epoch10\", device)\n",
    "evaluate_mt5_model(best_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0108d-c4e8-415c-9831-df97f2c48f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "files = os.listdir(checkpoint_dir)\n",
    "print(\"Obsah priečinka checkpoints:\")\n",
    "for file in files:\n",
    "    print(f\"- {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a00b7-347a-46b3-8f6a-912ea704ff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from checkpoints_full/mt5_toxicity_full_epoch10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Vyhodnocovanie:   1%|          | 7/625 [00:11<16:58,  1.65s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MT5ForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# Load full dataset\n",
    "train_df = pd.read_csv('toxic_eng/train.csv')\n",
    "test_df = pd.read_csv('toxic_eng/test.csv')\n",
    "\n",
    "train_texts = train_df['comment_text'].tolist()\n",
    "train_labels = train_df['toxic'].tolist()\n",
    "test_texts = test_df['comment_text'].tolist()\n",
    "test_labels = test_df['toxic'].tolist()\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "def load_best_model(model_class, checkpoint_dir, model_name, device):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}.pt\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading best model from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model = model_class.from_pretrained(\"google/mt5-base\", num_labels=2)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Best checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "def evaluate_mt5_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Vyhodnocovanie\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"\\nPresnosť na testovacích dátach: {torch.sum(torch.tensor(all_preds) == torch.tensor(all_labels)).item() / len(all_labels):.4f}\")\n",
    "    print(\"\\nKlasifikačná správa:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = load_best_model(MT5ForSequenceClassification, \"checkpoints_full\", \"mt5_toxicity_full_epoch10\", device)\n",
    "\n",
    "evaluate_mt5_model(best_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4745d150-934d-41a7-9269-e3757b8ceca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
